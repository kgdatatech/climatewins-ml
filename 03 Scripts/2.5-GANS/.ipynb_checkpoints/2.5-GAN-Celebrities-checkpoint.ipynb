{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c50e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, BatchNormalization, Input, GlobalAveragePooling2D, Dense, Reshape, LeakyReLU, Conv2DTranspose\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building the discriminator layers\n",
    "def build_discriminator(start_filters, spatial_dim, filter_size):\n",
    "        \n",
    "    # function for building a CNN block for downsampling the image\n",
    "    def add_discriminator_block(x, filters, filter_size):\n",
    "        x = Conv2D(filters, filter_size, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(filters, filter_size, padding='same', strides=2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x) #Can try other values\n",
    "        return x\n",
    "    \n",
    "    # input is an image with shape spatial_dim x spatial_dim and 3 channels\n",
    "    inp = Input(shape=(spatial_dim, spatial_dim, 3))\n",
    "\n",
    "    # design the discrimitor to downsample the image 4x\n",
    "    x = add_discriminator_block(inp, start_filters, filter_size)\n",
    "    x = add_discriminator_block(x, start_filters * 2, filter_size)\n",
    "    x = add_discriminator_block(x, start_filters * 4, filter_size)\n",
    "    x = add_discriminator_block(x, start_filters * 8, filter_size)\n",
    "    \n",
    "    # average and return a binary output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='LeakyReLU')(x) #Can also try sigmoid\n",
    "    \n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(start_filters, filter_size, latent_dim):\n",
    "          \n",
    "    # function for building a CNN block for upsampling the image\n",
    "    def add_generator_block(x, filters, filter_size):\n",
    "        x = Conv2DTranspose(filters, filter_size, strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x) #Can try other values\n",
    "        return x\n",
    "\n",
    "    # input is a noise vector \n",
    "    inp = Input(shape=(latent_dim,))\n",
    "\n",
    "    # projection of the noise vector into a tensor with \n",
    "    # same shape as last conv layer in discriminator\n",
    "    x = Dense(4 * 4 * (start_filters * 8), input_dim=latent_dim)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Reshape(target_shape=(4, 4, start_filters * 8))(x)\n",
    "\n",
    "    # design the generator to upsample the image 4x\n",
    "    x = add_generator_block(x, start_filters * 4, filter_size)\n",
    "    x = add_generator_block(x, start_filters * 2, filter_size)\n",
    "    x = add_generator_block(x, start_filters, filter_size)\n",
    "    x = add_generator_block(x, start_filters, filter_size)    \n",
    "\n",
    "    # turn the output into a 3D tensor, an image with 3 channels \n",
    "    x = Conv2D(3, kernel_size=5, padding='same', activation='tanh')(x) #tanh\n",
    "  \n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = #YOUR PATH HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdeaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load celebrity images attributes\n",
    "df_celeb = pd.read_csv(os.path.join(path, 'list_attr_celeba.csv'))\n",
    "TOTAL_SAMPLES = df_celeb.shape[0]\n",
    "\n",
    "# we will downscale the images\n",
    "SPATIAL_DIM = 64 \n",
    "# size of noise vector\n",
    "LATENT_DIM_GAN = 100 \n",
    "# filter size in conv layer\n",
    "FILTER_SIZE = 5\n",
    "# number of filters in conv layer\n",
    "NET_CAPACITY = 16\n",
    "# batch size\n",
    "BATCH_SIZE_GAN = 32\n",
    "# interval for displaying generated images\n",
    "PROGRESS_INTERVAL = 80 \n",
    "# directory for storing generated images\n",
    "ROOT_DIR = 'visualization'\n",
    "if not os.path.isdir(ROOT_DIR):\n",
    "    os.mkdir(ROOT_DIR)\n",
    "    \n",
    "\n",
    "\n",
    "def construct_models(verbose=False):\n",
    "    ### discriminator\n",
    "    discriminator = build_discriminator(NET_CAPACITY, SPATIAL_DIM, FILTER_SIZE)\n",
    "    # compile discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002), metrics=['mae']) #Can try other values for lr\n",
    "\n",
    "    ### generator\n",
    "    # do not compile generator\n",
    "    generator = build_generator(NET_CAPACITY, FILTER_SIZE, LATENT_DIM_GAN)\n",
    "\n",
    "    ### DCGAN \n",
    "    gan = Sequential()\n",
    "    gan.add(generator)\n",
    "    gan.add(discriminator)\n",
    "    discriminator.trainable = False \n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002), metrics=['mae']) #Can try other values for lr\n",
    "\n",
    "    if verbose: \n",
    "        generator.summary()\n",
    "        discriminator.summary()\n",
    "        gan.summary()\n",
    "        \n",
    "    return generator, discriminator, gan\n",
    "  \n",
    "generator_celeb, discriminator_celeb, gan_celeb = construct_models(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd208425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of discriminator updates per alternating training iteration\n",
    "DISC_UPDATES = 1  #Can try other values \n",
    "# number of generator updates per alternating training iteration\n",
    "GEN_UPDATES = 2 #Can try other values\n",
    "\n",
    "# function for training a GAN\n",
    "def run_training(generator, discriminator, gan, df=df_celeb, start_it=0, num_epochs=1000):\n",
    "        \n",
    "    # helper function for selecting 'size' real images\n",
    "    # and downscaling them to lower dimension SPATIAL_DIM\n",
    "    def get_real_images(df, size, total): #get_real_celebrity\n",
    "        cur_files = df.sample(frac=1).iloc[0:size]\n",
    "        X = np.empty(shape=(size, SPATIAL_DIM, SPATIAL_DIM, 3))\n",
    "        for i in range(0, size):\n",
    "            file = cur_files.iloc[i]\n",
    "            img0 = Image.open(os.path.join(path,'img_align_celeba', 'img_align_celeba',  file.image_id))\n",
    "            img = img0.resize((SPATIAL_DIM, SPATIAL_DIM))\n",
    "            img = np.flip(img, axis=2)\n",
    "            img = img[:,:,::-1]\n",
    "            img = img.astype(np.float32) / 127.5 - 1.0\n",
    "            X[i] = img\n",
    "        return X\n",
    "  \n",
    "    # list for storing loss\n",
    "    avg_loss_discriminator = []\n",
    "    avg_loss_generator = []\n",
    "    total_it = start_it\n",
    "\n",
    "    # main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # alternating training loop\n",
    "        loss_discriminator = []\n",
    "        loss_generator = []\n",
    "        for it in range(200): \n",
    "            \n",
    "            #### Discriminator training loop ####\n",
    "            for i in range(DISC_UPDATES): \n",
    "                # select a random set of real images\n",
    "                imgs_real = get_real_images(df, BATCH_SIZE_GAN, TOTAL_SAMPLES)\n",
    "                # generate a set of random noise vectors\n",
    "                noise = np.random.randn(BATCH_SIZE_GAN, LATENT_DIM_GAN)\n",
    "                # generate a set of fake images using the generator\n",
    "                imgs_fake = generator.predict(noise)\n",
    "                # train the discriminator on real images with label 1\n",
    "                d_loss_real = discriminator.train_on_batch(imgs_real, np.ones([BATCH_SIZE_GAN]))[1]\n",
    "                # train the discriminator on fake images with label 0\n",
    "                d_loss_fake = discriminator.train_on_batch(imgs_fake, np.zeros([BATCH_SIZE_GAN]))[1]\n",
    "\n",
    "            # display some fake images for visual control of convergence\n",
    "            if total_it % PROGRESS_INTERVAL == 0:\n",
    "                plt.figure(figsize=(5,2))\n",
    "                num_vis = min(BATCH_SIZE_GAN, 5)\n",
    "                imgs_real = get_real_images(df, num_vis, TOTAL_SAMPLES)\n",
    "                noise = np.random.randn(num_vis, LATENT_DIM_GAN)\n",
    "                imgs_fake = generator.predict(noise)\n",
    "                for obj_plot in [imgs_fake, imgs_real]:\n",
    "                    plt.figure(figsize=(num_vis * 3, 3))\n",
    "                    for b in range(num_vis):\n",
    "                        disc_score = float(discriminator.predict(np.expand_dims(obj_plot[b], axis=0))[0])\n",
    "                        plt.subplot(1, num_vis, b + 1)\n",
    "                        plt.title(str(round(disc_score, 3)))\n",
    "                        plt.imshow(obj_plot[b] * 0.5 + 0.5) \n",
    "                    if obj_plot is imgs_fake:\n",
    "                        plt.savefig(os.path.join(ROOT_DIR, str(total_it).zfill(10) + '.jpg'), format='jpg', bbox_inches='tight')\n",
    "                    plt.show()  \n",
    "\n",
    "            #### Generator training loop ####\n",
    "            loss = 0\n",
    "            y = np.ones([BATCH_SIZE_GAN, 1]) \n",
    "            for j in range(GEN_UPDATES):\n",
    "                # generate a set of random noise vectors\n",
    "                noise = np.random.randn(BATCH_SIZE_GAN, LATENT_DIM_GAN)\n",
    "                # train the generator on fake images with label 1\n",
    "                loss += gan.train_on_batch(noise, y)[1]\n",
    "\n",
    "            # store loss\n",
    "            loss_discriminator.append((d_loss_real + d_loss_fake) / 2.)        \n",
    "            loss_generator.append(loss / GEN_UPDATES)\n",
    "            total_it += 1\n",
    "\n",
    "        # visualize loss\n",
    "        clear_output(True)\n",
    "        print('Epoch', epoch)\n",
    "        avg_loss_discriminator.append(np.mean(loss_discriminator))\n",
    "        avg_loss_generator.append(np.mean(loss_generator))\n",
    "        plt.plot(range(len(avg_loss_discriminator)), avg_loss_discriminator)\n",
    "        plt.plot(range(len(avg_loss_generator)), avg_loss_generator)\n",
    "        plt.legend(['discriminator loss', 'generator loss'])\n",
    "        plt.show()\n",
    "\n",
    "    return generator, discriminator, gan\n",
    "\n",
    "generator_celeb, discriminator_celeb, gan_celeb = run_training(generator_celeb, \n",
    "                                                               discriminator_celeb, \n",
    "                                                               gan_celeb, \n",
    "                                                               num_epochs=500, \n",
    "                                                               df=df_celeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
